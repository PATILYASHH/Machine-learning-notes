### Write a Note on Bagging (Bootstrap Aggregation) – 4 Marks

**Definition:**  
Bagging, short for **Bootstrap Aggregation**, is an **ensemble learning technique** used to improve the **accuracy and stability** of Machine Learning models.  
It works by combining the results of **multiple models** trained on **different bootstrap samples** of the same dataset.

---

### **Key Points:**

1. **Bootstrap Sampling:**  
   - Multiple subsets of the dataset are created **randomly with replacement**.  
   - Each subset is used to train a separate model (usually of the same type).

2. **Model Training:**  
   - Each model learns slightly different patterns because it sees a **different portion** of the data.

3. **Aggregation Step:**  
   - After training, the predictions from all models are **aggregated** (by **voting** in classification or **averaging** in regression) to get the **final output**.

4. **Purpose:**  
   - To **reduce overfitting** and **variance** of models.  
   - Improves prediction stability and performance.

---

### **Example:**  
In a **Random Forest**, multiple decision trees are trained on different bootstrap samples, and their predictions are **aggregated** to produce the final result.

---

✅ **In short:**  
Bagging (Bootstrap Aggregation) is a **powerful ensemble method** that enhances model accuracy by **training multiple models on resampled data** and combining their predictions.